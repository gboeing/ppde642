{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Python/Pandas Refresher\n",
    "\n",
    "Overview of today's topics:\n",
    "\n",
    "  1. Quick Python refresher\n",
    "  1. pandas overview\n",
    "  1. Load data files\n",
    "  1. Select, filter, and slice data from a dataset\n",
    "  1. Merging and concatenating datasets\n",
    "  1. Grouping and summarizing data\n",
    "  1. Vectorization, map, and apply\n",
    "  1. Hierarchical indexing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Quick Python Refresher\n",
    "\n",
    "A quick overview of ubiquitous programming concepts including data types, for loops, if-then-else conditionals, and functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# integers (int)\n",
    "x = 100\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# floating-point numbers (float)\n",
    "x = 100.5\n",
    "type(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sequence of characters (str)\n",
    "x = 'Los Angeles, CA 90089'\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# list of items\n",
    "x = [1, 2, 3, 'USC']\n",
    "len(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sets are unique\n",
    "x = {2, 2, 3, 3, 1}\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuples are immutable sequences\n",
    "latlng = (34.019425, -118.283413)\n",
    "type(latlng)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can unpack a tuple\n",
    "lat, lng = latlng\n",
    "type(lat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dictionary of key:value pairs\n",
    "person = {'first_name': 'Geoff', 'last_name': 'Boeing', 'employer': 'USC'}\n",
    "type(person)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can convert types\n",
    "x = '100'\n",
    "print(type(x))\n",
    "y = int(x)\n",
    "print(type(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can loop through an iterable, such as a list or tuple\n",
    "for coord in latlng:\n",
    "    print('Current coordinate is:', coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loop through a dictionary keys and values as tuples\n",
    "for key, value in person.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# booleans are trues/falses\n",
    "x = 101\n",
    "x > 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use two == for equality and one = for assignment\n",
    "x == 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if, elif, else for conditional branching execution\n",
    "x = 101\n",
    "if x > 100:\n",
    "    print('Value is greater than 100.')\n",
    "elif x < 100:\n",
    "    print('Value is less than 100.')\n",
    "else:\n",
    "    print('Value is 100.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use functions to encapsulate and reuse bits of code\n",
    "def convert_items(my_list, new_type=str):\n",
    "    # convert each item in a list to a new type\n",
    "    new_list = [new_type(item) for item in my_list]\n",
    "    return new_list\n",
    "\n",
    "l = [1, 2, 3, 4]\n",
    "convert_items(l)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. pandas Series and DataFrames\n",
    "\n",
    "[pandas](https://pandas.pydata.org/) has two primary data structures we will work with: Series and DataFrames\n",
    "\n",
    "### 2a. pandas Series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a pandas series is based on a numpy array: it's fast, compact, and has more functionality\n",
    "# perhaps most notably, it has an index which allows you to work naturally with tabular data\n",
    "my_list = [8, 5, 77, 2]\n",
    "my_series = pd.Series(my_list)\n",
    "my_series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at a list-representation of the index\n",
    "my_series.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at the series' values themselves\n",
    "my_series.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the data type of the series' values?\n",
    "type(my_series.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what's the data type of the individual values themselves?\n",
    "my_series.dtype"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2b. pandas DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a dict can contain multiple lists and label them\n",
    "my_dict = {'hh_income'  : [75125, 22075, 31950, 115400],\n",
    "           'home_value' : [525000, 275000, 395000, 985000]}\n",
    "my_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a pandas dataframe can contain one or more columns\n",
    "# each column is a pandas series\n",
    "# each row is a pandas series\n",
    "# you can create a dataframe by passing in a list, array, series, or dict\n",
    "df = pd.DataFrame(my_dict)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the row labels in the index are accessed by the .index attribute of the DataFrame object\n",
    "df.index.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the column labels are accessed by the .columns attribute of the DataFrame object\n",
    "df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the data values are accessed by the .values attribute of the DataFrame object\n",
    "# this is a numpy (two-dimensional) array\n",
    "df.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Loading data\n",
    "\n",
    "In practice, you'll work with data by loading a dataset file into pandas. CSV is the most common format. But pandas can also ingest tab-separated data, JSON, and proprietary file formats like Excel .xlsx files, Stata, SAS, and SPSS.\n",
    "\n",
    "Below, notice what pandas's `read_csv` function does:\n",
    "\n",
    "1. recognize the header row and get its variable names\n",
    "1. read all the rows and construct a pandas DataFrame (an assembly of pandas Series rows and columns)\n",
    "1. construct a unique index, beginning with zero\n",
    "1. infer the data type of each variable (ie, column)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load a data file\n",
    "# note the relative filepath! where is this file located?\n",
    "# use dtype argument if you don't want pandas to guess your data types\n",
    "df = pd.read_csv('../../data/world_cities.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataframe shape as rows, columns\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or use len to just see the number of rows\n",
    "len(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the dataframe's \"head\"\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# view the dataframe's \"tail\"\n",
    "df.tail()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Selecting and slicing data from a DataFrame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CHEAT SHEET OF COMMON TASKS\n",
    "# Operation                       Syntax           Result\n",
    "#------------------------------------------------------------\n",
    "# Select column by name           df[col]          Series\n",
    "# Select columns by name          df[col_list]     DataFrame\n",
    "# Select row by label             df.loc[label]    Series\n",
    "# Select row by integer location  df.iloc[loc]     Series\n",
    "# Slice rows by label             df.loc[a:c]      DataFrame\n",
    "# Select rows by boolean vector   df[mask]         DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4a. Select DataFrame's column(s) by name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select a single column by column name\n",
    "# this is a pandas series\n",
    "df['resident_pop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select multiple columns by a list of column names\n",
    "# this is a pandas dataframe that is a subset of the original\n",
    "df[['resident_pop', 'built_up_area']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a new column by assigning df['new_col'] to some values\n",
    "df['pop_density'] = df['resident_pop'] / df['built_up_area']\n",
    "\n",
    "# you can do vectorized math operations on any numeric columns\n",
    "df['pop_density_1000s'] = df['pop_density'] / 1000\n",
    "\n",
    "# inspect the results\n",
    "df[['resident_pop', 'built_up_area', 'pop_density', 'pop_density_1000s']].head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4b. Select row(s) by label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .loc to select by row label\n",
    "# returns the row as a series whose index is the dataframe column names\n",
    "df.loc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use .loc to select single value by row label, column name\n",
    "df.loc[0, 'resident_pop']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice of rows from label 5 to label 7, inclusive\n",
    "# this returns a pandas dataframe\n",
    "df.loc[5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# slice of rows from label 5 to label 7, inclusive\n",
    "# slice of columns from uc_names to world_subregion, inclusive\n",
    "df.loc[1:3, 'uc_names':'world_subregion']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subset of rows from with labels in list\n",
    "# subset of columns with names in list\n",
    "df.loc[[1, 3], ['uc_names', 'world_subregion']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can use a column of identifiers as the index (indices do not *need* to be unique)\n",
    "# uc_id values uniquely identify each row (but verify!)\n",
    "df = df.set_index('uc_id')\n",
    "df.index.is_unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# .loc works by label, not by position in the dataframe\n",
    "try:\n",
    "    df.loc[0]\n",
    "except KeyError as e:\n",
    "    print('label not found')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the index now contains uc_id values, so you have to use .loc accordingly to select by row label\n",
    "df.loc[14]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4c. Select by (integer) position"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the row in the zero-th position in the dataframe\n",
    "df.iloc[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can slice as well\n",
    "# note, while .loc is inclusive, .iloc is not\n",
    "# get the rows from position 0 up to but not including position 3 (ie, rows 0, 1, and 2)\n",
    "df.iloc[0:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the value from the row in position 3 and the column in position 2 (zero-indexed)\n",
    "df.iloc[3, 2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4d. Select/filter by value\n",
    "\n",
    "You can subset or filter a dataframe for based on the values in its rows/columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# filter the dataframe by urban areas with more than 25 million residents\n",
    "df[df['resident_pop'] > 25000000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what exactly did that do? let's break it out.\n",
    "df['resident_pop'] > 25000000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# essentially a true/false mask that filters by value\n",
    "mask = df['resident_pop'] > 25000000\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can chain multiple conditions together\n",
    "# pandas logical operators are: | for or, & for and, ~ for not\n",
    "# these must be grouped by using parentheses due to order of operations\n",
    "mask = (df['resident_pop'] > 25000000) & (df['built_up_area'] > 2000)\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which urban areas have 25 million residents and either 2000 km2 area or >200 meter avg elevation?\n",
    "mask1 = df['resident_pop'] > 25000000\n",
    "mask2 = df['built_up_area'] > 2000\n",
    "mask3 = df['avg_elevation'] > 200\n",
    "mask = mask1 & (mask2 | mask3)\n",
    "df[mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see the mask\n",
    "mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~ means not... it essentially flips trues to falses and vice-versa\n",
    "~mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# which urban areas contain a city with \"New \" in its name?\n",
    "mask = df['uc_names'].str.contains('New ')\n",
    "df.loc[mask, ['uc_names', 'country']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# create a new subset dataframe containing all urban areas in the US with >5 million residents\n",
    "# how many rows did you get?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Merge and concatenate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5a. Merging DataFrames\n",
    "\n",
    "Merging joins data together, aligned by the index. Assume here that we had two separate data sets, with unique `uc_id` identifiers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subset dataframe with climate related variables\n",
    "climate_cols = ['core_city', 'avg_elevation', 'avg_precipitation', 'avg_temperature', 'climate_classes']\n",
    "df_climate = df[climate_cols].sample(2000).sort_values('avg_temperature', ascending=True)\n",
    "df_climate.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a subset dataframe with economic variables\n",
    "econ_cols = ['core_city', 'gdp_ppp', 'night_light_em', 'un_income_class']\n",
    "df_econ = df[econ_cols].sample(2000).sort_values('gdp_ppp', ascending=False)\n",
    "df_econ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge them together, aligning rows based on their labels in the index\n",
    "df_merged = pd.merge(left=df_econ, right=df_climate, how='inner', left_index=True, right_index=True)\n",
    "df_merged.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset df_econ's index\n",
    "df_econ = df_econ.reset_index()\n",
    "df_econ.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# merge them together, aligning rows based on left's column values and right's index labels\n",
    "df_merged = pd.merge(left=df_econ, right=df_climate, how='inner', left_on='uc_id', right_index=True)\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inner join: only retain rows where a match is found in both dataframes\n",
    "# left join: retain all rows in left and bring in attributes from rows that matched in right\n",
    "# right join: retain all rows in right and bring in attributes from rows that matched in left\n",
    "# outer join: retain all rows in both dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5b. Concatenating DataFrames\n",
    "\n",
    "Concatenating smushes data together along some axis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create two subset dataframes\n",
    "df_us = df[df['country'] == 'united_states']\n",
    "df_uk = df[df['country'] == 'united_kingdom']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concatenate them together\n",
    "df_us_uk = pd.concat([df_us, df_uk], sort=False)\n",
    "df_us_uk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Grouping and summarizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate per capita GDP then group the rows by region\n",
    "df['gdp_percap'] = df['gdp_ppp'] / df['resident_pop']\n",
    "groups = df.groupby('world_subregion')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# what is the median per capita GDP across the urban areas in each region?\n",
    "groups['gdp_percap'].median().sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# look at several columns' medians by region\n",
    "groups[['gdp_percap', 'avg_temperature', 'pop_density']].median()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# re-group the urban areas by country and find the highest then lowest urban area avg elevation in each country"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Vectorization, Map, and Apply\n",
    "\n",
    "Avoid slow iteration in pandas! Use vectorization (best) or map/apply (ok) when possible. These methods apply a function across elements, rows, and columns of a pandas DataFrame or Series. But they have some important and often confusing differences. Some tips:\n",
    "\n",
    "  - avoid .iterrows() always\n",
    "  - use vectorization wherever possible\n",
    "  - .map() applies a function element-wise on a Series\n",
    "  - .apply() works on a row or column basis on a DataFrame (specify the axis!), or element-wise on a Series\n",
    "  - .applymap() works element-wise on an entire DataFrame\n",
    "\n",
    "Let's see what that means in practice with some examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate resident population z-scores, vectorized\n",
    "z = (df['resident_pop'] - df['resident_pop'].mean()) / df['resident_pop'].std()\n",
    "z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.map()` applies a function element-wise on a series"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df['resident_pop'].mean()\n",
    "std = df['resident_pop'].std()\n",
    "def calculate_zscore(x, mean=mean, std=std):\n",
    "    return (x - mean) / std\n",
    "    \n",
    "# map the function to the series\n",
    "z = df['resident_pop'].map(calculate_zscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A `lambda` function is a simple, one-off, anonymous function. You can't call it again later because it doesn't have a name. It just lets you repeatedly perform some operation across a series of values (in our case, a column in our dataframe) using a minimal amount of code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean = df['resident_pop'].mean()\n",
    "std = df['resident_pop'].std()\n",
    "\n",
    "# map a lambda function to the series\n",
    "z = df['resident_pop'].map(lambda x: (x - mean) / std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "# check timings of vectorized vs map\n",
    "z = (df['resident_pop'] - df['resident_pop'].mean()) / df['resident_pop'].std()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%timeit\n",
    "mean = df['resident_pop'].mean()\n",
    "std = df['resident_pop'].std()\n",
    "z = df['resident_pop'].map(lambda x: (x - mean) / std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.apply()` is like `.map()`, but it works on a row or column basis on an entire DataFrame (specify the axis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the difference between the min and max values in each column (ie, row-wise)\n",
    "df_subset = df[['area', 'built_up_area', 'avg_elevation']]\n",
    "df_subset.apply(lambda col: col.max() - col.min(), axis='rows')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# find the difference between the min and max values in each row (ie, column-wise)\n",
    "df_subset.apply(lambda row: row.max() - row.min(), axis='columns')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# how would you vectorize the above .apply() methods to make the calculation faster?\n",
    "# then compare the timings of your vectorized version to the .apply version above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`.applymap()` works element-wise on an entire DataFrame. This is like doing a `.map()` to each column in the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this uses applymap, but you could (should) vectorize it\n",
    "# we'll see that next week\n",
    "df_subset = df[['country', 'world_region', 'world_subregion']]\n",
    "df_subset.applymap(lambda x: x.upper().replace('_', ' '))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Hierarchical Indexing\n",
    "\n",
    "A pandas index can comprise multiple values. It's very powerful, but gets tricky sometimes. See [the docs](https://pandas.pydata.org/pandas-docs/stable/user_guide/advanced.html#multiindex-advanced-indexing).\n",
    "\n",
    "In general, it's a good idea to sort your index for better querying performance later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index().set_index(['country', 'core_city']).sort_index()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# index isn't unique, but doesn't need to be\n",
    "# you could make it unique by adding uc_id as a 3rd level, for instance\n",
    "df.index.is_unique"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what's the benefit of multi-indexing like this? Your dataframe becomes a very fast look-up table. A good index is one that gives you a useful handle to select rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select all urban areas in china (ie, first index level)\n",
    "df.loc['china']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# or select rows by multiple index levels\n",
    "# lots of unnamed core cities in china in this dataset\n",
    "df.loc[('china', 'unnamed')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select every row with an unnamed core city in the dataset\n",
    "# that is, only select by the 2nd level of the index\n",
    "# the first : slices everything in the first index level, and the trailing : slices all columns\n",
    "df.loc[pd.IndexSlice[:, ['unnamed']], :]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# select every row in argentina or spain with core city of cordoba\n",
    "df.loc[(['argentina', 'spain'], ['cordoba']), :]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stacking, unstacking, and pivoting are common ways to reshape data with a MultiIndex. Depending on the form your original data come in, they may be useful in the data prep phase to make your data easier to analyze. We'll discuss data prep, cleaning, and (review) visualization next week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now it's your turn\n",
    "# multi-index the dataframe by world region, subregion, and uc_id\n",
    "# use .loc to select all the rows from southern africa and eastern europe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (ppd599)",
   "language": "python",
   "name": "ppd599"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
